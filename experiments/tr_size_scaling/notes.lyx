#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
MPS Pure state training
\end_layout

\begin_layout Standard
The training scheme used here is that outlined in 
\begin_inset CommandInset href
LatexCommand href
name "this paper"
target "https://arxiv.org/abs/1712.03213"

\end_inset

 from Lei Wang.
 The state is modeled as a pure-state MPS with unspecified bond dimension,
 defined on a system of size 
\begin_inset Formula $L$
\end_inset

.
 Samples are drawn from the ground-truth state 
\begin_inset Formula $\Psi$
\end_inset

 in a sequence of random bases: at each measurement step, a set of 
\begin_inset Formula $L$
\end_inset

 angles 
\begin_inset Formula $(\theta,\phi)$
\end_inset

, randomly distributed across the unit sphere, is used to construct 
\begin_inset Formula $L$
\end_inset

 random single-qubit unitaries that are applied to the ground-truth state;
 a single measurement is then taken in the 
\begin_inset Formula $z$
\end_inset

 product basis.
 This procedure is repeated 
\begin_inset Formula $N$
\end_inset

 times to construct a dataset consisting of applied unitaries and corresponding
 outcomes.
 
\end_layout

\begin_layout Standard
The MPSs are trained by stochastic gradient descent using the two-site update
 method described in the paper above: for a particular batch of data, the
 algorithm sweeps back and forth across the MPS, adjusting the local tensors
 at each bond in order to raise the log-likelihood assigned by the MPS to
 the observed data.
 At each update step, the local bond dimension of the MPS can be adjusted
 according to user-specified cutoffs; a regularization term proportional
 to the Renyi-2 entropy of the state when cut across each bond is applied.
\end_layout

\begin_layout Section
Preliminary experiments
\end_layout

\begin_layout Standard
I trained MPS models on ground-truth states constructed as MPSs with fixed
 bond dimension and random-normal values in each tensor.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename plots/fidelity_vs_N_scaling_L.pdf
	width 50page%

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
Fidelity onto ground-truth state for various system sizes.
 Error bars correspond to different random seeds.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "f-scaling-L"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "f-scaling-L"

\end_inset

 plots the fidelity of the trained model onto the ground truth state as
 a function of training set size; prior to saturation the behavior is approximat
ely linear in the number of samples
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename plots/fidelity_threshold_vs_L.pdf
	width 50page%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Number of samples required to acheive fidelity .99
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "f-thresh"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "f-thresh"

\end_inset

 I plot the number of training samples required to acheive a fidelity of
 .99, as a function of system size; the dependence is surprisingly weak (I'm
 using a low bond dimension here so perhaps the targets are 'too easy').
\end_layout

\begin_layout Standard
I have made no attempts to optimize the training, which is done via vanilla
 SGD; in particular the hyperparameters were set by hand without any sort
 of cross-validation.
 There is significant variation across different random model seeds.
\end_layout

\end_body
\end_document
